# ğŸ¤– Private Local Agentic HR Assistant

A **Secure, Local-First, Cloud-Enhanced Agentic RAG System** designed for HR operations. This project allows organizations to process sensitive HR policy queries using **Local LLMs via Ollama**, while leveraging **Pinecone Hybrid Search (Sparse + Dense)** for scalable cloud retrieval.

---

## ğŸ“Œ Key Highlights

*  **Local LLM Reasoning** (Ollama) â†’ Full privacy
*  **Hybrid RAG** â†’ Dense + Sparse search for maximum accuracy
*  **Modular Architecture** â†’ Clean and scalable
*  **Production Ready** â†’ Designed for real HR environments
*  **PDF Knowledge Base** â†’ Fully indexed HR policies

---

## ğŸ—ï¸ System Architecture

The application follows a **Modular Monolith** design with clear separation of responsibilities.

### **Architecture Components**

| Component         | Technology          | Purpose                           |
| ----------------- | ------------------- | --------------------------------- |
| **LLM Engine**    | Ollama (Llama 3.2)  | Local inferenceâ€”private and fast  |
| **Vector DB**     | Pinecone Serverless | Hybrid search (Sparse + Dense)    |
| **Embeddings**    | nomic-embed-text    | Highâ€‘quality 768â€‘dim text vectors |
| **Orchestration** | LangChain           | RAG pipeline + tools integration  |
| **Backend**       | Flask (Python)      | REST API + App Logic              |
| **Memory**        | Redis               | User session & chat history       |
| **Frontend**      | HTML / CSS / JS     | Modern, responsive UI             |

### **Directory Structure**

```
HR-Assistant-Chatbot
â”‚
â”œâ”€â”€ core/                  # ğŸ§  The Brain (Logic Layer)
â”‚   â”œâ”€â”€ config.py          # Central Config & Env Variables
â”‚   â”œâ”€â”€ rag_pipeline.py    # Hybrid RAG Logic
â”‚   â”œâ”€â”€ chat_memory.py     # Redis-Based Memory Manager
â”‚   â””â”€â”€ ingest.py          # ETL + Hybrid Indexing
â”‚
â”œâ”€â”€ static/                # ğŸ¨ Frontend Assets
â”‚   â”œâ”€â”€ css/style.css
â”‚   â””â”€â”€ js/main.js
â”‚
â”œâ”€â”€ templates/             # ğŸ“„ HTML Templates
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ data/                  # ğŸ“‚ Source Documents
â”‚   â””â”€â”€ *.pdf              # HR Policies
â”‚
â”œâ”€â”€ vectorstore/           # âš™ï¸ Sparse Values
â”‚   â””â”€â”€ bm25_values.json
â”‚
â”œâ”€â”€ app.py                 # ğŸš€ Application Entry Point
â””â”€â”€ requirements.txt       # Dependency List
```

---

## âš™ï¸ Installation & Setup

### **1. Prerequisites**

* Python 3.9+
* Pinecone serverless index (dimension: 768, metric: `dotproduct`)
* Redis (run locally: `redis-server`)
* Ollama installed (`ollama.com`)

---

### **2. Clone Repository**

```bash
git clone https://github.com/yourusername/hr-assistant-chatbot.git
cd hr-assistant-chatbot
```

---

### **3. Download Required LLM Models (Ollama)**

```bash
ollama pull llama3.2
ollama pull nomic-embed-text
```

---

### **4. Setup Virtual Environment**

```bash
python -m venv venv
source venv/bin/activate    # Windows: venv\Scripts\activate

pip install -r requirements.txt
```

---

### **5. Configure Environment Variables**

Create `.env` in project root:

```ini
# --- Flask Security ---
SECRET_KEY=your_super_secret_random_key

# --- Pinecone Vector DB ---
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=hr-policy-index  # Must use 'dotproduct' metric

# --- Redis Memory ---
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
```

---

## ğŸš€ Usage Guide

### **Step 1 â€” Ingest HR PDFs (Build Knowledge Base)**

This script reads all PDFs from `data/`, generates Hybrid (Dense + Sparse) vectors, and uploads them to Pinecone.

```bash
python -m core.ingest
```

**Output:**

```
ğŸ‰ Ingestion Complete! Vectors successfully uploaded.
```

---

### **Step 2 â€” Run the Application**

```bash
python app.py
```

Open in browser:

```
http://127.0.0.1:5000
```

Try queries such as:

* *"What is the sick leave policy?"*
* *"How many annual leaves are allowed?"*
* *"Is there a travel allowance in the company?"*

---

## ğŸ› ï¸ Troubleshooting

| Issue                        | Solution                                             |
| ---------------------------- | ---------------------------------------------------- |
| **Pinecone Metric Error**    | Use `dotproduct`, hybrid doesnâ€™t work with `cosine`. |
| **Ollama not responding**    | Ensure `ollama serve` is running.                    |
| **BM25 params missing**      | Run ingestion script once to generate JSON.          |
| **Redis connection refused** | Start Redis: `redis-server`.                         |

---

## ğŸ”® Future Roadmap

* [ ] Docker containerization (Full stack)
* [ ] Multi-agent routing with LangGraph
* [ ] Voice Interface via Whisper
* [ ] Admin dashboard for PDF uploads

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create your branch: `git checkout -b feature/NewFeature`
3. Commit changes
4. Push: `git push origin feature/NewFeature`
5. Submit a Pull Request

---

## ğŸ“œ License

Licensed under the **MIT License**.

Built with â¤ï¸ using Generative AI
